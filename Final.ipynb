{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adityamengade/C-Codes/blob/master/Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfKcTdEgX6d8",
        "colab_type": "code",
        "outputId": "96293ac7-6657-4448-e5c0-005d2086ceaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        }
      },
      "source": [
        "import keras\n",
        "import PIL\n",
        "import numpy as np\n",
        "import os\n",
        "from matplotlib.pyplot import imshow\n",
        "from PIL import Image\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense\n",
        "from keras import backend as K\n",
        "from keras import regularizers\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ReduceLROnPlateau,EarlyStopping\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "save_dir = os.path.join(os.getcwd(), 'cnn_model')\n",
        "model_name = 'cnn_model.h5'\n",
        "img_width, img_height = 128,128\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "        rotation_range=40,\n",
        "        width_shift_range=0.050,\n",
        "        height_shift_range=0.05,\n",
        "        rescale=1/255,\n",
        "        shear_range=0.1,\n",
        "        zoom_range=0.1,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest')\n",
        "train_data_dir = 'spiral/training'\n",
        "validation_data_dir = 'spiral/testing'\n",
        "nb_train_samples = 3000\n",
        "nb_validation_samples = 1150\n",
        "epochs = 50\n",
        "batch_size = 16\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    input_shape = (1, img_width, img_height)\n",
        "else:\n",
        "    input_shape = (img_width, img_height, 1)\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(64, (3,3),input_shape=(256,256, 1)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Conv2D(64,(3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/content/drive/My Drive/spiral/training',\n",
        "        color_mode='grayscale', \n",
        "        target_size=(256,256),  \n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary')  \n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        '/content/drive/My Drive/spiral/testing',\n",
        "        color_mode='grayscale',\n",
        "        target_size=(256,256),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary')\n",
        "reduce_lr_2 = ReduceLROnPlateau(monitor='val_loss',patience=6,rate=0.4,min_lr=1e-12,verbose=1)\n",
        "early_stop_2 = EarlyStopping(monitor='val_loss',patience=12,verbose=1)\n",
        "model.fit_generator(\n",
        "        train_generator,\n",
        "        steps_per_epoch=2000 // batch_size,\n",
        "        epochs=20,\n",
        "        shuffle = True,\n",
        "        validation_data=validation_generator,\n",
        "        validation_steps=400 // batch_size,\n",
        "        callbacks=[reduce_lr_2,early_stop_2],verbose=1)\n",
        "model.save_weights('cnn_model.h5')\n",
        "\n",
        "\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "model_path = os.path.join(save_dir, model_name)\n",
        "model.save(model_path)\n",
        "print('Saved trained model at %s ' % model_path)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "Found 832 images belonging to 2 classes.\n",
            "Found 344 images belonging to 2 classes.\n",
            "Epoch 1/20\n",
            "62/62 [==============================] - 14s 223ms/step - loss: 0.6694 - acc: 0.5706 - val_loss: 0.6616 - val_acc: 0.5346\n",
            "Epoch 2/20\n",
            "62/62 [==============================] - 13s 209ms/step - loss: 0.5934 - acc: 0.6522 - val_loss: 0.6490 - val_acc: 0.5452\n",
            "Epoch 3/20\n",
            "62/62 [==============================] - 13s 214ms/step - loss: 0.5561 - acc: 0.6689 - val_loss: 0.8729 - val_acc: 0.5160\n",
            "Epoch 4/20\n",
            "62/62 [==============================] - 13s 217ms/step - loss: 0.5624 - acc: 0.6759 - val_loss: 0.7077 - val_acc: 0.5452\n",
            "Epoch 5/20\n",
            "62/62 [==============================] - 13s 211ms/step - loss: 0.5445 - acc: 0.6724 - val_loss: 0.6967 - val_acc: 0.5984\n",
            "Epoch 6/20\n",
            "62/62 [==============================] - 14s 218ms/step - loss: 0.5435 - acc: 0.6719 - val_loss: 0.9133 - val_acc: 0.5426\n",
            "Epoch 7/20\n",
            "62/62 [==============================] - 13s 211ms/step - loss: 0.5374 - acc: 0.6900 - val_loss: 1.1371 - val_acc: 0.5612\n",
            "Epoch 8/20\n",
            "62/62 [==============================] - 13s 214ms/step - loss: 0.5481 - acc: 0.6704 - val_loss: 0.9094 - val_acc: 0.5585\n",
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 9/20\n",
            "62/62 [==============================] - 13s 215ms/step - loss: 0.5386 - acc: 0.6744 - val_loss: 0.8724 - val_acc: 0.5851\n",
            "Epoch 10/20\n",
            "62/62 [==============================] - 13s 213ms/step - loss: 0.5387 - acc: 0.6815 - val_loss: 1.1026 - val_acc: 0.5372\n",
            "Epoch 11/20\n",
            "62/62 [==============================] - 13s 212ms/step - loss: 0.5330 - acc: 0.6779 - val_loss: 1.0245 - val_acc: 0.5435\n",
            "Epoch 12/20\n",
            "62/62 [==============================] - 13s 202ms/step - loss: 0.5286 - acc: 0.6789 - val_loss: 1.1070 - val_acc: 0.5426\n",
            "Epoch 13/20\n",
            "62/62 [==============================] - 13s 213ms/step - loss: 0.5286 - acc: 0.6830 - val_loss: 1.1920 - val_acc: 0.5559\n",
            "Epoch 14/20\n",
            "62/62 [==============================] - 13s 210ms/step - loss: 0.5335 - acc: 0.6784 - val_loss: 1.1004 - val_acc: 0.5612\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "Epoch 00014: early stopping\n",
            "Saved trained model at /content/cnn_model/cnn_model.h5 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMhdW1I9BTna",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FM1F610nQvsK",
        "colab_type": "code",
        "outputId": "198a5d96-0879-4f50-a82f-7e18856074cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "source": [
        "import cv2\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/',force_remount=True)\n",
        "img= image.load_img('/content/drive/My Drive/h.png', target_size=(150,150))\n",
        "print(img)\n",
        "x = image.img_to_array(img)\n",
        "x = np.expand_dims(x, axis=0)\n",
        "x = preprocess_input(x)\n",
        "print(\"result\")\n",
        "res=model.predict(x)\n",
        "print(res)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n",
            "<PIL.Image.Image image mode=RGB size=150x150 at 0x7F60BA1E8748>\n",
            "result\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-1e02b0c70a43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"result\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mres\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m         \u001b[0;31m# Case 2: Symbolic tensors or Numpy array-like.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1380\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1381\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    755\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    139\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected conv2d_1_input to have shape (150, 150, 1) but got array with shape (150, 150, 3)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YwnHCwiYAzX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model.save('cnn_model.h5')\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model_file('cnn_model.h5')\n",
        "tflite_model = converter.convert()\n",
        "open('cnn_model.tflite', 'wb').write(tflite_model)\n",
        "\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download('cnn_model.tflite')\n",
        "except:\n",
        "    print(\"Skip downloading\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KT4KE5zP4VI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from keras.utils import plot_model\n",
        "plot_model(model,to_file=\"a.png\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8oIQgdCU5LR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vjgwUu_hGfR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Dw-_y4P2Z5K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAsORrC62ee4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}